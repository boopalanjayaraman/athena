{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS tagging - NLTK",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hq3JpykkqrJR"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwsYxE2lq1cM",
        "outputId": "9e641d99-256e-4615-c595-9a2ab9d747ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "2CR66PaQrNRD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all') # or 'popular'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vMJqDBcrQTr",
        "outputId": "570d0660-8760-4555-9def-deea2e0bbcdf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/extended_omw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "yTJqLQhgrRl0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "CC: Coordinating conjunction\n",
        "CD: Cardinal number\n",
        "DT: Determiner\n",
        "EX: Existential there\n",
        "FW: Foreign word\n",
        "IN: Preposition or subordinating conjunction\n",
        "JJ: Adjective\n",
        "VP: Verb Phrase\n",
        "JJR: Adjective, comparative\n",
        "JJS: Adjective, superlative\n",
        "LS: List item marker\n",
        "MD: Modal\n",
        "NN: Noun, singular or mass\n",
        "NNS: Noun, plural\n",
        "PP: Preposition Phrase\n",
        "NNP: Proper noun, singular Phrase\n",
        "NNPS: Proper noun, plural\n",
        "PDT: Pre determiner\n",
        "POS: Possessive ending\n",
        "PRP: Personal pronoun Phrase\n",
        "PRP: Possessive pronoun Phrase\n",
        "RB: Adverb\n",
        "RBR: Adverb, comparative\n",
        "RBS: Adverb, superlative\n",
        "RP: Particle\n",
        "S: Simple declarative clause\n",
        "SBAR: Clause introduced by a (possibly empty) subordinating conjunction\n",
        "SBARQ: Direct question introduced by a wh-word or a wh-phrase.\n",
        "SINV: Inverted declarative sentence, i.e. one in which the subject follows the tensed verb or modal.\n",
        "SQ: Inverted yes/no question, or main clause of a wh-question, following the wh-phrase in SBARQ.\n",
        "SYM: Symbol\n",
        "VBD: Verb, past tense\n",
        "VBG: Verb, gerund or present participle\n",
        "VBN: Verb, past participle\n",
        "VBP: Verb, non-3rd person singular present\n",
        "VBZ: Verb, 3rd person singular present\n",
        "WDT: Wh-determiner\n",
        "WP: Wh-pronoun\n",
        "WP: Possessive wh-pronoun\n",
        "WRB: Wh-adverb\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "dyOzcDDhCOaT",
        "outputId": "cace85de-b4a4-4f4d-cdde-e8ff1b1ae77b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nCC: Coordinating conjunction\\nCD: Cardinal number\\nDT: Determiner\\nEX: Existential there\\nFW: Foreign word\\nIN: Preposition or subordinating conjunction\\nJJ: Adjective\\nVP: Verb Phrase\\nJJR: Adjective, comparative\\nJJS: Adjective, superlative\\nLS: List item marker\\nMD: Modal\\nNN: Noun, singular or mass\\nNNS: Noun, plural\\nPP: Preposition Phrase\\nNNP: Proper noun, singular Phrase\\nNNPS: Proper noun, plural\\nPDT: Pre determiner\\nPOS: Possessive ending\\nPRP: Personal pronoun Phrase\\nPRP: Possessive pronoun Phrase\\nRB: Adverb\\nRBR: Adverb, comparative\\nRBS: Adverb, superlative\\nRP: Particle\\nS: Simple declarative clause\\nSBAR: Clause introduced by a (possibly empty) subordinating conjunction\\nSBARQ: Direct question introduced by a wh-word or a wh-phrase.\\nSINV: Inverted declarative sentence, i.e. one in which the subject follows the tensed verb or modal.\\nSQ: Inverted yes/no question, or main clause of a wh-question, following the wh-phrase in SBARQ.\\nSYM: Symbol\\nVBD: Verb, past tense\\nVBG: Verb, gerund or present participle\\nVBN: Verb, past participle\\nVBP: Verb, non-3rd person singular present\\nVBZ: Verb, 3rd person singular present\\nWDT: Wh-determiner\\nWP: Wh-pronoun\\nWP: Possessive wh-pronoun\\nWRB: Wh-adverb\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_tags = '''commented/VBD purchasing/VBG said/VBD operated/VBN follow/VB accepted/VBN merges/VBZ buys/VBZ acquires/VBZ announces/VBZ'''\n",
        "[nltk.tag.str2tuple(t) for t in new_tags.split()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8wDxkr6C581",
        "outputId": "475f8bf9-8f8c-4721-e834-3a5c8ada8e98"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('commented', 'VBD'),\n",
              " ('purchasing', 'VBG'),\n",
              " ('said', 'VBD'),\n",
              " ('operated', 'VBN'),\n",
              " ('follow', 'VB'),\n",
              " ('accepted', 'VBN'),\n",
              " ('merges', 'VBZ'),\n",
              " ('buys', 'VBZ'),\n",
              " ('acquires', 'VBZ'),\n",
              " ('announces', 'VBZ')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Amazon invests $2B to Expand Operations in India\"\n",
        "\n",
        "tokenized = sent_tokenize(text)\n",
        "print(tokenized)\n",
        "for sentence_token in tokenized:\n",
        "     \n",
        "    # Word tokenizers is used to find the words\n",
        "    # and punctuation in a string\n",
        "    wordsList = nltk.word_tokenize(sentence_token)\n",
        "    wordsList = [w for w in wordsList if not w in stop_words]\n",
        "    tagged = nltk.pos_tag(wordsList)\n",
        " \n",
        "    print(tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm_XpHkXsPUG",
        "outputId": "ecac6e35-6fe1-4c3d-cd1c-c878841372e2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Amazon invests $2B to Expand Operations in India']\n",
            "[('Amazon', 'NNP'), ('invests', 'VBZ'), ('$', '$'), ('2B', 'CD'), ('Expand', 'NNP'), ('Operations', 'NNP'), ('India', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_sentence(text):\n",
        "  pos_sentences = []\n",
        "\n",
        "  tokenized = sent_tokenize(text)\n",
        "  for token in tokenized:   \n",
        "    # Word tokenizers is used to find the words\n",
        "    # and punctuation in a string\n",
        "    wordsList = nltk.word_tokenize(token)\n",
        "    wordsList = [w for w in wordsList if not w in stop_words]\n",
        "    tagged = nltk.pos_tag(wordsList)\n",
        "    pos_sentences.append(tagged)\n",
        "\n",
        "  return pos_sentences"
      ],
      "metadata": {
        "id": "T4sJsQOP-N8u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_pos_sentence(\"Agilent to acquire BioTek for $1.165B\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG_H0nF__VqP",
        "outputId": "c0b1a6ab-1457-479f-e921-40facc59eba9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Agilent', 'NNP'),\n",
              "  ('acquire', 'VB'),\n",
              "  ('BioTek', 'NNP'),\n",
              "  ('$', '$'),\n",
              "  ('1.165B', 'CD')]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_pos_sentence(\"Broadcom Corporation (BRCM) President & CEO Scott A Mcgregor sells 20,000 Shares\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LcY3Y-T_Z6R",
        "outputId": "4fe4d2e7-c927-4149-9e74-e24334c5d463"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Broadcom', 'NNP'),\n",
              "  ('Corporation', 'NNP'),\n",
              "  ('(', '('),\n",
              "  ('BRCM', 'NNP'),\n",
              "  (')', ')'),\n",
              "  ('President', 'NNP'),\n",
              "  ('&', 'CC'),\n",
              "  ('CEO', 'NNP'),\n",
              "  ('Scott', 'NNP'),\n",
              "  ('A', 'NNP'),\n",
              "  ('Mcgregor', 'NNP'),\n",
              "  ('sells', 'VBZ'),\n",
              "  ('20,000', 'CD'),\n",
              "  ('Shares', 'NNS')]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_pos_sentence(\"Alternative Investment Allocations Should increase In 2017 Across The Board Due To Market Uncertainty\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5CHlx34_j0a",
        "outputId": "a116e833-1649-43fe-a857-676eea4c651e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Alternative', 'JJ'),\n",
              "  ('Investment', 'NNP'),\n",
              "  ('Allocations', 'NNP'),\n",
              "  ('Should', 'NNP'),\n",
              "  ('increase', 'NN'),\n",
              "  ('In', 'IN'),\n",
              "  ('2017', 'CD'),\n",
              "  ('Across', 'IN'),\n",
              "  ('The', 'DT'),\n",
              "  ('Board', 'NNP'),\n",
              "  ('Due', 'NNP'),\n",
              "  ('To', 'TO'),\n",
              "  ('Market', 'NNP'),\n",
              "  ('Uncertainty', 'NNP')]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_pos_sentence(\"Bulldog Investors, LLC buys Korea Equity Fund, Stellar Acquisition III Inc, New York REIT Inc, ...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3Qwo-kE_swd",
        "outputId": "f9031938-74f6-45bd-e77d-5602033acaea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Bulldog', 'NNP'),\n",
              "  ('Investors', 'NNPS'),\n",
              "  (',', ','),\n",
              "  ('LLC', 'NNP'),\n",
              "  ('buys', 'VBZ'),\n",
              "  ('Korea', 'NNP'),\n",
              "  ('Equity', 'NNP'),\n",
              "  ('Fund', 'NNP'),\n",
              "  (',', ','),\n",
              "  ('Stellar', 'NNP'),\n",
              "  ('Acquisition', 'NNP'),\n",
              "  ('III', 'NNP'),\n",
              "  ('Inc', 'NNP'),\n",
              "  (',', ','),\n",
              "  ('New', 'NNP'),\n",
              "  ('York', 'NNP'),\n",
              "  ('REIT', 'NNP'),\n",
              "  ('Inc', 'NNP'),\n",
              "  (',', ','),\n",
              "  ('...', ':')]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_pos_sentence(\"Bragar Eagel & Squire, P.C. announces That a Class Action Lawsuit Has Been Filed Against ...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VStzzCOO_7as",
        "outputId": "69ae9123-bdd4-4387-8b26-68f1fc51da43"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Bragar', 'NNP'),\n",
              "  ('Eagel', 'NNP'),\n",
              "  ('&', 'CC'),\n",
              "  ('Squire', 'NNP'),\n",
              "  (',', ','),\n",
              "  ('P.C', 'NNP'),\n",
              "  ('.', '.')],\n",
              " [('announces', 'NNS'),\n",
              "  ('That', 'WDT'),\n",
              "  ('Class', 'NNP'),\n",
              "  ('Action', 'NNP'),\n",
              "  ('Lawsuit', 'NNP'),\n",
              "  ('Has', 'NNP'),\n",
              "  ('Been', 'NNP'),\n",
              "  ('Filed', 'VBN'),\n",
              "  ('Against', 'NNP'),\n",
              "  ('...', ':')]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_pos_sentence(\"Sun Pharma to develop chikungunya, zika drugs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk6Upw3hAEhk",
        "outputId": "de79c9be-f938-4163-b427-e86e9c3432dc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Sun', 'NNP'),\n",
              "  ('Pharma', 'NNP'),\n",
              "  ('develop', 'VB'),\n",
              "  ('chikungunya', 'NN'),\n",
              "  (',', ','),\n",
              "  ('zika', 'FW'),\n",
              "  ('drugs', 'NNS')]]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "get_pos_sentence(\"Walmart looks to exit Japan with Seiyu supermarket sale Nikkei\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMZCiiHaAVJZ",
        "outputId": "0def7edc-3c07-49d5-e636-877bbec77022"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Walmart', 'NNP'),\n",
              "  ('looks', 'VBZ'),\n",
              "  ('exit', 'NN'),\n",
              "  ('Japan', 'NNP'),\n",
              "  ('Seiyu', 'NNP'),\n",
              "  ('supermarket', 'NN'),\n",
              "  ('sale', 'NN'),\n",
              "  ('Nikkei', 'NNP')]]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_pos_sentence(\"Aaron's acquires Crusader - Analyst Blog\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCDTEcOiAaTZ",
        "outputId": "79cf28c4-c20b-4d44-8a6f-25685aad6aba"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Aaron', 'NNP'),\n",
              "  (\"'s\", 'POS'),\n",
              "  ('acquires', 'NNS'),\n",
              "  ('Crusader', 'NNP'),\n",
              "  ('-', ':'),\n",
              "  ('Analyst', 'NN'),\n",
              "  ('Blog', 'NN')]]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_pos_sentence(\"Satyam merges with TechMahindra\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfdGfYk9Ag8n",
        "outputId": "6b86e194-5e02-4d59-8c8d-dbb2e98175ec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Satyam', 'NNP'), ('merges', 'NNS'), ('TechMahindra', 'NNP')]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp_pipeline = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "JqPeXlL4AuZv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp_pipeline(\"Sun Pharma to develop chikungunya, zika drugs\")\n",
        "for token in doc3: \n",
        "   print(token,\"|\", token.pos_ , \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsRxxNOrYKKH",
        "outputId": "acd5f0cc-ba0c-4aa8-8db2-0b49348b7d70"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun | PROPN | Sun\n",
            "Pharma | PROPN | Pharma\n",
            "to | PART | to\n",
            "develop | VERB | develop\n",
            "chikungunya | PROPN | chikungunya\n",
            ", | PUNCT | ,\n",
            "zika | PROPN | zika\n",
            "drugs | NOUN | drug\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp_pipeline(\"Marcato Capital Management Llc buys Univar Inc, Astec Industries Inc, IAC/InterActiveCorp, ...\")\n",
        "for token in doc3: \n",
        "   print(token,\"|\", token.pos_ , \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0RZjDGBYdq3",
        "outputId": "caa4c8f5-e669-4a81-a6b9-ed95d2e37914"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Marcato | PROPN | Marcato\n",
            "Capital | PROPN | Capital\n",
            "Management | PROPN | Management\n",
            "Llc | PROPN | Llc\n",
            "buys | VERB | buy\n",
            "Univar | PROPN | Univar\n",
            "Inc | PROPN | Inc\n",
            ", | PUNCT | ,\n",
            "Astec | PROPN | Astec\n",
            "Industries | PROPN | Industries\n",
            "Inc | PROPN | Inc\n",
            ", | PUNCT | ,\n",
            "IAC | PROPN | IAC\n",
            "/ | SYM | /\n",
            "InterActiveCorp | PROPN | InterActiveCorp\n",
            ", | PUNCT | ,\n",
            "... | PUNCT | ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp_pipeline(\"Bragar Eagel & Squire, P.C. Announces That a Class Action Lawsuit Has Been Filed Against ...\")\n",
        "for token in doc3: \n",
        "  print(token,\"|\", token.pos_ , \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc22K7-7YlIu",
        "outputId": "f3189b9d-5117-4bc5-e30c-3491677a3cd8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bragar | PROPN | Bragar\n",
            "Eagel | PROPN | Eagel\n",
            "& | CCONJ | &\n",
            "Squire | PROPN | Squire\n",
            ", | PUNCT | ,\n",
            "P.C. | PROPN | P.C.\n",
            "Announces | VERB | announce\n",
            "That | SCONJ | that\n",
            "a | DET | a\n",
            "Class | PROPN | Class\n",
            "Action | PROPN | Action\n",
            "Lawsuit | PROPN | Lawsuit\n",
            "Has | AUX | have\n",
            "Been | AUX | be\n",
            "Filed | VERB | file\n",
            "Against | ADP | against\n",
            "... | PUNCT | ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp_pipeline(\"Amazon invests $2B to expand operations in India\")\n",
        "for token in doc3: \n",
        "  print(token,\"|\", token.pos_ , \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBTDAERpZQZI",
        "outputId": "de22f072-4e6b-4901-bd15-588b04147946"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amazon | PROPN | Amazon\n",
            "invests | VERB | invest\n",
            "$ | SYM | $\n",
            "2B | NOUN | 2B\n",
            "to | PART | to\n",
            "expand | VERB | expand\n",
            "operations | NOUN | operation\n",
            "in | ADP | in\n",
            "India | PROPN | India\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp_pipeline(\"Amazon to open checkout free store in New York\")\n",
        "for token in doc3: \n",
        "  print(token,\"|\", token.pos_ , \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eiN-W58Zb7F",
        "outputId": "91a84d41-f48f-4948-aaa9-caad5df6ef52"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amazon | PROPN | Amazon\n",
            "to | PART | to\n",
            "open | VERB | open\n",
            "checkout | NOUN | checkout\n",
            "free | ADJ | free\n",
            "store | NOUN | store\n",
            "in | ADP | in\n",
            "New | PROPN | New\n",
            "York | PROPN | York\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp_pipeline(\"Aaron's acquires Crusader - Analyst Blog\")\n",
        "for token in doc3: \n",
        "  print(token,\"|\", token.pos_ , \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgcHQsLfZh4_",
        "outputId": "a6b9ea4c-b3b3-4210-c3f2-45cbf3a02cd0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aaron | PROPN | Aaron\n",
            "'s | PART | 's\n",
            "acquires | VERB | acquire\n",
            "Crusader | PROPN | Crusader\n",
            "- | PUNCT | -\n",
            "Analyst | PROPN | Analyst\n",
            "Blog | PROPN | Blog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp_pipeline(\"OneConnect Financial Files For U.S. IPO Amid Steep Losses\")\n",
        "for token in doc3: \n",
        "  print(token,\"|\", token.pos_ , \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI7rTiApZnSe",
        "outputId": "b3b5fb0d-a28d-47c1-d5c5-62f7ad301d2a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OneConnect | PROPN | OneConnect\n",
            "Financial | ADJ | financial\n",
            "Files | NOUN | file\n",
            "For | ADP | for\n",
            "U.S. | PROPN | U.S.\n",
            "IPO | NOUN | ipo\n",
            "Amid | ADP | amid\n",
            "Steep | PROPN | Steep\n",
            "Losses | NOUN | loss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp_pipeline(\"oneconnect financial files for u.s. ipo amid steep losses\")\n",
        "for token in doc3: \n",
        "  print(token,\"|\", token.pos_ , \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mrRJ9aKbj-z",
        "outputId": "fa7adf76-5181-4d79-f186-b21d8501f13b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oneconnect | ADJ | oneconnect\n",
            "financial | ADJ | financial\n",
            "files | NOUN | file\n",
            "for | ADP | for\n",
            "u.s | PROPN | u.s\n",
            ". | PROPN | .\n",
            "ipo | PROPN | ipo\n",
            "amid | ADP | amid\n",
            "steep | ADJ | steep\n",
            "losses | NOUN | loss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aidifZI8c6oG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}